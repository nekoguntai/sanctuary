# =============================================
# Sanctuary Bitcoin Wallet - Docker Compose
# =============================================
#
# IMPORTANT: This application is designed to run HTTPS-ONLY.
# HTTP requests on port 80 are automatically redirected to HTTPS.
# This is required for WebUSB hardware wallet support (browser security).
#
# Usage:
#   HTTPS_PORT=8443 JWT_SECRET="your-secret" docker compose up -d
#
# Environment Variables:
#   - HTTPS_PORT: HTTPS port (default: 443, use 8443 for non-root)
#   - HTTP_PORT: HTTP redirect port (default: 80)
#   - JWT_SECRET: Required for authentication (must be set)
#   - LOG_LEVEL: Logging verbosity - debug, info, warn, error (default: info)
#   - GATEWAY_PORT: Mobile API gateway port (default: 4000)
#
# SSL Certificates:
#   Self-signed certs are in ./docker/nginx/ssl/
#   For production, replace with real certificates.
#
# Mobile App Gateway:
#   The gateway container provides API access for iOS/Android apps.
#   It runs on port 4000 by default. See gateway/README.md for details.
#
# =============================================

services:
  # ============================================
  # PostgreSQL Database
  # ============================================
  postgres:
    image: postgres:16-alpine
    container_name: sanctuary-db
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-sanctuary}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD must be set}
      POSTGRES_DB: ${POSTGRES_DB:-sanctuary}
    # Port exposure disabled by default for security
    # Uncomment for local development or use: docker exec -it sanctuary-db psql -U sanctuary
    # ports:
    #   - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-sanctuary} -d ${POSTGRES_DB:-sanctuary}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    networks:
      - sanctuary-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    # Prevent swap thrashing - kill rather than swap excessively
    mem_swappiness: 10
    memswap_limit: 768M  # memory + 256M swap

  # ============================================
  # Backend API Server
  # ============================================
  backend:
    build:
      context: ./server
      dockerfile: Dockerfile
    image: sanctuary-backend:local
    container_name: sanctuary-backend
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      # Server
      NODE_ENV: ${NODE_ENV:-production}
      PORT: 3001
      API_URL: ${API_URL:-http://localhost:3001}
      CLIENT_URL: ${CLIENT_URL:-http://localhost}

      # Logging (debug, info, warn, error)
      LOG_LEVEL: ${LOG_LEVEL:-info}

      # Database
      DATABASE_URL: postgresql://${POSTGRES_USER:-sanctuary}:${POSTGRES_PASSWORD:-sanctuary}@postgres:5432/${POSTGRES_DB:-sanctuary}?schema=public

      # JWT
      JWT_SECRET: ${JWT_SECRET:?JWT_SECRET is required}
      JWT_EXPIRES_IN: ${JWT_EXPIRES_IN:-7d}

      # Rate limiting (set to 100 for testing)
      LOGIN_RATE_LIMIT: ${LOGIN_RATE_LIMIT:-5}
      PASSWORD_CHANGE_RATE_LIMIT: ${PASSWORD_CHANGE_RATE_LIMIT:-5}

      # Encryption (for sensitive data like node passwords)
      ENCRYPTION_KEY: ${ENCRYPTION_KEY:?ENCRYPTION_KEY is required (at least 32 chars)}

      # Bitcoin Network
      BITCOIN_NETWORK: ${BITCOIN_NETWORK:-mainnet}

      # Electrum Server
      ELECTRUM_HOST: ${ELECTRUM_HOST:-electrum.blockstream.info}
      ELECTRUM_PORT: ${ELECTRUM_PORT:-50002}
      ELECTRUM_PROTOCOL: ${ELECTRUM_PROTOCOL:-ssl}

      # Price APIs
      MEMPOOL_API: ${MEMPOOL_API:-https://mempool.space/api/v1}
      COINGECKO_API: ${COINGECKO_API:-https://api.coingecko.com/api/v3}
      KRAKEN_API: ${KRAKEN_API:-https://api.kraken.com/0/public}

      # AI Container URL (isolated AI service)
      # Backend forwards all AI requests to this container
      AI_CONTAINER_URL: ${AI_CONTAINER_URL:-http://ai:3100}

      # Docker proxy for secure Ollama container management
      DOCKER_PROXY_URL: http://docker-proxy:2375

      # AI config sync secret (must match ai container)
      AI_CONFIG_SECRET: ${AI_CONFIG_SECRET:-sanctuary-ai-config-secret}
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - sanctuary-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    mem_swappiness: 10
    memswap_limit: 1536M  # memory + 512M swap

  # ============================================
  # Database Migrations & Seeding (one-time run)
  # Uses the same image as backend (no rebuild)
  # ============================================
  migrate:
    image: sanctuary-backend:local
    container_name: sanctuary-migrate
    depends_on:
      backend:
        condition: service_started
      postgres:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql://${POSTGRES_USER:-sanctuary}:${POSTGRES_PASSWORD:-sanctuary}@postgres:5432/${POSTGRES_DB:-sanctuary}?schema=public
    command: ["sh", "-c", "npx prisma migrate deploy && npx prisma db seed"]
    networks:
      - sanctuary-network
    restart: "no"
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # ============================================
  # Frontend (Nginx + React) - HTTPS ONLY
  # ============================================
  # This frontend is configured for HTTPS-only operation:
  # - Port 80 (HTTP) redirects to HTTPS
  # - Port 443 (HTTPS) serves the application
  # - SSL certificates mounted from ./docker/nginx/ssl/
  # - ENABLE_SSL=true is required for WebUSB hardware wallet support
  # ============================================
  frontend:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        VITE_API_URL: ${VITE_API_URL:-}
    container_name: sanctuary-frontend
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
    environment:
      BACKEND_HOST: backend
      BACKEND_PORT: 3001
      ENABLE_SSL: "true"
      HTTPS_REDIRECT_PORT: ${HTTPS_PORT:-443}
    ports:
      - "${HTTP_PORT:-80}:80"
      - "${HTTPS_PORT:-443}:443"
    volumes:
      - ./docker/nginx/ssl:/etc/nginx/ssl:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "--no-check-certificate", "https://localhost:443/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - sanctuary-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    mem_swappiness: 10
    memswap_limit: 384M  # memory + 128M swap

  # ============================================
  # Mobile API Gateway
  # ============================================
  # Public-facing gateway for mobile app access.
  # Handles authentication, rate limiting, and proxying to backend.
  # Push notifications via FCM (Android) and APNs (iOS).
  #
  # Push notifications auto-enable when credentials are provided:
  #   - FCM: Mount fcm-service-account.json from Firebase Console
  #   - APNs: Mount apns-key.p8 + set APNS_KEY_ID, APNS_TEAM_ID, APNS_BUNDLE_ID
  # ============================================
  gateway:
    build:
      context: ./gateway
      dockerfile: Dockerfile
    image: sanctuary-gateway:local
    container_name: sanctuary-gateway
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
    environment:
      NODE_ENV: ${NODE_ENV:-production}
      PORT: 4000

      # Backend connection (internal)
      BACKEND_URL: http://backend:3001
      BACKEND_WS_URL: ws://backend:3001

      # JWT (must match backend)
      JWT_SECRET: ${JWT_SECRET:?JWT_SECRET is required}

      # Rate limiting
      RATE_LIMIT_WINDOW_MS: ${GATEWAY_RATE_LIMIT_WINDOW_MS:-60000}
      RATE_LIMIT_MAX_REQUESTS: ${GATEWAY_RATE_LIMIT_MAX:-60}

      # Apple Push Notification Service (iOS)
      # Auto-enabled when APNS_KEY_ID, APNS_TEAM_ID, and key file are provided
      APNS_KEY_ID: ${APNS_KEY_ID:-}
      APNS_TEAM_ID: ${APNS_TEAM_ID:-}
      APNS_BUNDLE_ID: ${APNS_BUNDLE_ID:-}
      APNS_PRODUCTION: ${APNS_PRODUCTION:-false}
    ports:
      - "${GATEWAY_PORT:-4000}:4000"
    volumes:
      # Mount FCM service account (optional) - create this file from Firebase Console
      - ${FCM_SERVICE_ACCOUNT:-/dev/null}:/app/config/fcm-service-account.json:ro
      # Mount APNs key (optional) - create this file from Apple Developer Portal
      - ${APNS_KEY_FILE:-/dev/null}:/app/config/apns-key.p8:ro
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - sanctuary-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    mem_swappiness: 10
    memswap_limit: 384M  # memory + 128M swap

  # ============================================
  # AI Container (Isolated AI Service)
  # ============================================
  # Security-isolated container for AI operations.
  # Lightweight - idles until AI is enabled in Admin â†’ AI Assistant.
  #
  # ISOLATION GUARANTEES:
  # - Cannot access database directly
  # - Cannot access private keys or signing
  # - Only receives sanitized transaction metadata
  # - Backend NEVER makes external AI calls
  #
  # NETWORK MODES (default: local AI only):
  # - Remove ai-internal network below to allow internet access for cloud AI
  # ============================================
  ai:
    build:
      context: ./ai-proxy
      dockerfile: Dockerfile
    image: sanctuary-ai:local
    container_name: sanctuary-ai
    restart: unless-stopped
    depends_on:
      backend:
        condition: service_healthy
    environment:
      NODE_ENV: ${NODE_ENV:-production}
      PORT: 3100
      # Backend URL for fetching sanitized data
      BACKEND_URL: http://backend:3001
      # AI config sync secret (must match backend)
      AI_CONFIG_SECRET: ${AI_CONFIG_SECRET:-sanctuary-ai-config-secret}
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:3100/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    networks:
      - sanctuary-network
      # Add ai-internal for local-only mode (no internet)
      # Remove this line to allow internet access for cloud AI
      - ai-internal
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 64M
    mem_swappiness: 10
    memswap_limit: 384M  # memory + 128M swap

  # ============================================
  # Docker Socket Proxy (Security Layer)
  # ============================================
  # Secure proxy for Docker socket access.
  # Only allows specific operations needed for Ollama management:
  # - List containers (to check if Ollama is running)
  # - Start/stop containers (to manage Ollama)
  # Blocks all other Docker API calls (no creating containers, volumes, etc.)
  # ============================================
  docker-proxy:
    image: tecnativa/docker-socket-proxy:latest
    container_name: sanctuary-docker-proxy
    restart: unless-stopped
    environment:
      # Only allow container inspection and start/stop
      CONTAINERS: 1
      POST: 1
      # Block everything else
      IMAGES: 0
      NETWORKS: 0
      VOLUMES: 0
      EXEC: 0
      SWARM: 0
      NODES: 0
      SERVICES: 0
      TASKS: 0
      SECRETS: 0
      CONFIGS: 0
      PLUGINS: 0
      SYSTEM: 0
      DISTRIBUTION: 0
      AUTH: 0
      BUILD: 0
      COMMIT: 0
      EVENTS: 0
      INFO: 0
      VERSION: 0
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - sanctuary-network
    # Run with minimal privileges
    privileged: false
    read_only: true
    tmpfs:
      - /run
      - /tmp
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 64M
        reservations:
          cpus: '0.05'
          memory: 32M
    mem_swappiness: 0  # Proxy should never swap
    memswap_limit: 64M  # No swap for proxy

  # ============================================
  # Ollama LLM Server (Optional - for bundled AI)
  # ============================================
  # Local LLM server bundled with Sanctuary.
  # Only starts when ENABLE_OLLAMA=true (set via ./start.sh --with-ai)
  #
  # This eliminates the need to:
  # - Install Ollama on your host machine
  # - Configure network addresses
  # - Manually connect Sanctuary to Ollama
  #
  # GPU Support (optional):
  #   For NVIDIA GPUs, uncomment the deploy section below.
  #   Requires: nvidia-container-toolkit installed on host.
  #
  # Models are persisted in the ollama_data volume.
  # ============================================
  ollama:
    image: ollama/ollama:latest
    container_name: sanctuary-ollama
    restart: unless-stopped
    profiles:
      - ai
    environment:
      # Listen on all interfaces so ai-proxy can connect
      OLLAMA_HOST: 0.0.0.0
    volumes:
      # Persist downloaded models
      - ollama_data:/root/.ollama
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - sanctuary-network
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    # Resource limits for Ollama (LLMs need significant memory)
    # Uncomment reservations.devices for NVIDIA GPU support
    deploy:
      resources:
        limits:
          cpus: '4'
          memory: 8G
        reservations:
          cpus: '1'
          memory: 2G
          # Uncomment for NVIDIA GPU support:
          # devices:
          #   - driver: nvidia
          #     count: all
          #     capabilities: [gpu]
    # LLMs benefit from swap for model loading
    mem_swappiness: 60
    memswap_limit: 12G  # memory + 4G swap for model flexibility

# ============================================
# Networks
# ============================================
networks:
  sanctuary-network:
    driver: bridge

  # Internal network for AI - no internet access
  # AI container joins this network by default for security
  # To allow cloud AI, remove ai-internal from AI container's networks
  ai-internal:
    driver: bridge
    internal: true

# ============================================
# Volumes
# ============================================
volumes:
  postgres_data:
    driver: local
  ollama_data:
    driver: local
